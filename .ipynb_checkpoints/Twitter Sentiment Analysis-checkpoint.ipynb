{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965bf207-9e3f-4bdf-86d5-958b025d5e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.10.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (3.2.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.9.24)\n",
      "Requirement already satisfied: textblob in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (2022.9.13)\n",
      "Requirement already satisfied: click in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (1.23.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (3.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (4.37.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\bharg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install textblob\n",
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001d2bb0-908a-4750-9e53-2cafa203d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bharg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bharg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bharg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bharg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f56795c-38ec-4d2a-9bce-49929a9796dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['target', 'id', 'date','flag','user','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49730f9-b969-4122-b3b9-939b90397a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalDataDF = pd.read_csv('data/tweet_data.csv', names=colNames, delimiter=',' ,engine='python', nrows=None, encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f55b5e7-c313-4e14-9590-2aad7086121b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDataDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0732484f-7602-40a9-a6ff-d1627a54f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDataDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9097dd7d-54a5-4cdf-88c5-344dff47daf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDataDF.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd4f9b9-981e-4708-96ef-1fedc9b0d4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          is upset that he can't update his Facebook by ...\n",
       "2          @Kenichan I dived many times for the ball. Man...\n",
       "3            my whole body feels itchy and like its on fire \n",
       "4          @nationwideclass no, it's not behaving at all....\n",
       "                                 ...                        \n",
       "1599995    Just woke up. Having no school is the best fee...\n",
       "1599996    TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997    Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998    Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDataDF['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923056aa-53ff-42ef-8fb8-59e26166a69d",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b8739b-ae3f-4c20-b90a-0db1e9bc56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_emojis(text):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', text)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', text)\n",
    "    # Love -- <3, :*\n",
    "    text = re.sub(r'(<3|:\\*)', ' EMO_POS ', text)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', text)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', text)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3507b2e7-38cc-464b-af6c-ff44f76cfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the text\n",
    "\n",
    "# removing tagged username '@'\n",
    "def cleaningText(text):\n",
    "    text = text.strip('\\'\"?!,.():;') # removing punctuation\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text) # convert more than 2 letter repetitions to 2 letter #fooood -> food\n",
    "    text = re.sub(r'(-|\\')','',text) # removing additional -& '\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text) #removing @usernames\n",
    "    text = re.sub(r'#','',text) #removing '#' symbols\n",
    "    text = re.sub(r'RT[\\s]+','',text) #removes RT(Re-Tweet) string \n",
    "    text = re.sub(r'https?:\\/\\/\\S+','',text) #removing the hyperlink\n",
    "    text = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', text) #removing urls\n",
    "    # Replace 2+ dots with space\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    # Strip space, \" and ' from tweet\n",
    "    text = text.strip(' \"\\'')\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    text = handling_emojis(text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower() #make the text to lowercase\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df51804-ae13-4538-b2c1-c9f013ff6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aww, thats a bummer. you shoulda got david carr of third day to do it. emo_pos '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDataDF['text'] = originalDataDF['text'].apply(cleaningText)\n",
    "originalDataDF['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391b2b99-1d55-4a7a-a0ec-50dffc176237",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractionWords = {\n",
    "\"aren’t\":\"are not\",\"can’t\":\"can not\",\"couldn’t\":\"could not \",\"didn’t\":\"did not\",\"doesn’t\":\"does not\",\"don’t\":\"do not\",\"hadn’t\":\"had not\",\"hasn’t\":\"has not \",\"haven’t\":\"have not\",\n",
    "\"I’m\":\"I am\",\"I’ve\":\"I have\",\"isn’t\":\"is not\",\"let’s\":\"let us\",\"mightn’t\":\"might not\",\"mustn’t\":\"must not\",\"shan’t\":\"shall not\",\"shouldn’t\":\"should not\",\"that’s\":\" that is\",\"he’ll\":\" he will\",\n",
    "\"I’ll\":\"I will\",\"she’ll\":\"she will\",\"she’s\":\"she is\",\"there’s\":\"there is\",\"they’ll\":\" they will\",\"they’re\":\"they are\",\"they’ve\":\"they have\",\"we’re\":\"we are\",\"we’ve\":\"we have\",\"weren’t\":\"were not\",\n",
    "\"what’ll\":\"what will\",\"what’re\":\"what are\",\"what’ve\":\"what have\",\"where’s\":\"where is\",\"who’d\":\"who would\",\"who’ll\":\"who will\",\"who’re\":\"who are\",\"who’s\":\"who is\",\"who’ve\":\"who have\",\"won’t\":\"will not\",\n",
    "\"wouldn’t\":\"would not\",\"you’d\":\"you would\",\"you’re\":\"you are\",\"you’ve\":\"you have\",\"it’s\":\"it is\",\"wasn't\":\"was not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13a2d884-7bd7-4ff6-bfa5-ed66d75a74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negation handling\n",
    "def negationHandling(text):\n",
    "    words = text.split()\n",
    "    temp = [contractionWords[word] if word in contractionWords else word for word in words]\n",
    "    temp = \" \".join(temp)\n",
    "    return temp\n",
    "originalDataDF['text'] = originalDataDF['text'].apply(negationHandling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cdf1be7-e383-4755-a424-86e2d3db6b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aww',\n",
       " ',',\n",
       " 'thats',\n",
       " 'a',\n",
       " 'bummer',\n",
       " '.',\n",
       " 'you',\n",
       " 'shoulda',\n",
       " 'got',\n",
       " 'david',\n",
       " 'carr',\n",
       " 'of',\n",
       " 'third',\n",
       " 'day',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " '.',\n",
       " 'emo_pos']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = []\n",
    "def wordTokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "originalDataDF['text'] = originalDataDF['text'].apply(wordTokenize)\n",
    "originalDataDF['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19be6ab9-9034-4792-80f6-52109117563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aww',\n",
       " ',',\n",
       " 'thats',\n",
       " 'bummer',\n",
       " '.',\n",
       " 'shoulda',\n",
       " 'got',\n",
       " 'david',\n",
       " 'carr',\n",
       " 'third',\n",
       " 'day',\n",
       " '.',\n",
       " 'emo_pos']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def removeStopWords(tokens):\n",
    "    temp = [word for word in tokens if word not in stop_words]\n",
    "    return temp\n",
    "originalDataDF['text'] = originalDataDF['text'].apply(removeStopWords)\n",
    "originalDataDF['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2d4856-cb73-4472-9aa2-df2395ac5bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aww', 'thats', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeUnnecessaryChars(tokens):\n",
    "    temp = [word for word in tokens if word.isalpha()]\n",
    "    return temp\n",
    "originalDataDF['text'] = originalDataDF['text'].apply(removeUnnecessaryChars)\n",
    "originalDataDF['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac476a32-79da-466d-9667-5dd5552353e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b04915e-2739-44e2-8234-d6f531d657ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "def lemmatizeTweets(wordList):\n",
    "    temp = []\n",
    "    for word in wordList:\n",
    "        _word = lemma.lemmatize(word)\n",
    "        temp.append(_word)\n",
    "    return ' '.join(temp)\n",
    "originalDataDF['text'] = originalDataDF['text'].apply(lemmatizeTweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221fb56c-7d32-463e-a75d-70b4aa662ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whole body feel itchy like fire'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalDataDF['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9c244ce-d000-4b56-ba32-8d30b2f2653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "requiredTweetData = originalDataDF[[\"target\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b9f212-898c-4d94-a93b-a69e35c90fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking sample data for vectorization\n",
    "requiredTweetData = requiredTweetData[700000:900000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aacbd347-3961-4e6b-bfe7-ade13944d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "word_set = []\n",
    "\n",
    "for sent in requiredTweetData['text']:\n",
    "    temp = [i for i in word_tokenize(sent)]\n",
    "    sentences.append(temp)\n",
    "    for word in temp:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    "            \n",
    "word_set = set(word_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58dd8f1d-00b4-4056-9255-028917b61f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in :\n",
    "#         if word not in word_set:\n",
    "#             word_set.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4d64703-ccd3-4331-9f73-9befb0e7cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in requiredTweetData['text']:\n",
    "#     for word in sent:\n",
    "#         if word not in word_set:\n",
    "#             word_set.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ac4569f-7673-4707-b654-edb219f3de41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catching',\n",
       " 'cncled',\n",
       " 'wecome',\n",
       " 'smarticle',\n",
       " 'egads',\n",
       " 'mich',\n",
       " 'kugel',\n",
       " 'dow',\n",
       " 'tatiana',\n",
       " 'outfit',\n",
       " 'col',\n",
       " 'spiderr',\n",
       " 'yeaay',\n",
       " 'dcontroller',\n",
       " 'nthbridge',\n",
       " 'olmuyodu',\n",
       " 'isadora',\n",
       " 'paradox',\n",
       " 'canadia',\n",
       " 'bumped',\n",
       " 'whatsoever',\n",
       " 'rrowr',\n",
       " 'raht',\n",
       " 'watchnuts',\n",
       " 'nerdrage',\n",
       " 'published',\n",
       " 'patchy',\n",
       " 'moneyy',\n",
       " 'pierced',\n",
       " 'rah',\n",
       " 'bloodsugar',\n",
       " 'superb',\n",
       " 'ughh',\n",
       " 'sleepfilled',\n",
       " 'ohhaii',\n",
       " 'surreal',\n",
       " 'stripe',\n",
       " 'urrg',\n",
       " 'epilogue',\n",
       " 'geneticist',\n",
       " 'marvel',\n",
       " 'bite',\n",
       " 'doctarts',\n",
       " 'indpls',\n",
       " 'whidbey',\n",
       " 'garrarrarr',\n",
       " 'trods',\n",
       " 'shirley',\n",
       " 'oriented',\n",
       " 'sugarwise',\n",
       " 'herniated',\n",
       " 'nightline',\n",
       " 'malate',\n",
       " 'madagascar',\n",
       " 'omo',\n",
       " 'blackeye',\n",
       " 'rove',\n",
       " 'finger',\n",
       " 'dramed',\n",
       " 'vie',\n",
       " 'thet',\n",
       " 'sadder',\n",
       " 'nao',\n",
       " 'ryuishi',\n",
       " 'stosur',\n",
       " 'valpo',\n",
       " 'smackgobbed',\n",
       " 'ellu',\n",
       " 'bluemoon',\n",
       " 'drama',\n",
       " 'parachute',\n",
       " 'pratice',\n",
       " 'skill',\n",
       " 'wifeyy',\n",
       " 'forqot',\n",
       " 'pine',\n",
       " 'bigsky',\n",
       " 'se',\n",
       " 'clevland',\n",
       " 'shunned',\n",
       " 'wozniacki',\n",
       " 'alzheimers',\n",
       " 'bagus',\n",
       " 'carmallows',\n",
       " 'antwerpen',\n",
       " 'tudo',\n",
       " 'chlorine',\n",
       " 'oopss',\n",
       " 'mfor',\n",
       " 'gaintnerd',\n",
       " 'maxwell',\n",
       " 'eveningg',\n",
       " 'capn',\n",
       " 'marigold',\n",
       " 'fangame',\n",
       " 'dune',\n",
       " 'crapcrapcrap',\n",
       " 'shitt',\n",
       " 'devs',\n",
       " 'mufti',\n",
       " 'oprahs',\n",
       " 'urbannetwork',\n",
       " 'stoppen',\n",
       " 'pchow',\n",
       " 'monome',\n",
       " 'alangan',\n",
       " 'virgin',\n",
       " 'initial',\n",
       " 'batcave',\n",
       " 'sgm',\n",
       " 'lsa',\n",
       " 'gased',\n",
       " 'nonononnononoonono',\n",
       " 'cosette',\n",
       " 'wooo',\n",
       " 'michealson',\n",
       " 'schatz',\n",
       " 'niceness',\n",
       " 'lausanne',\n",
       " 'ethier',\n",
       " 'grrll',\n",
       " 'achieve',\n",
       " 'tailbone',\n",
       " 'ninety',\n",
       " 'yech',\n",
       " 'lidls',\n",
       " 'sweetee',\n",
       " 'asada',\n",
       " 'ragious',\n",
       " 'primero',\n",
       " 'misadventure',\n",
       " 'kerri',\n",
       " 'ngeliat',\n",
       " 'mayari',\n",
       " 'celeste',\n",
       " 'outgrown',\n",
       " 'agreeable',\n",
       " 'whenver',\n",
       " 'saber',\n",
       " 'dinglenemies',\n",
       " 'fabtastic',\n",
       " 'misunderstanding',\n",
       " 'malicious',\n",
       " 'complement',\n",
       " 'bek',\n",
       " 'katia',\n",
       " 'dorkie',\n",
       " 'miau',\n",
       " 'nemoree',\n",
       " 'gater',\n",
       " 'haja',\n",
       " 'jonases',\n",
       " 'sulking',\n",
       " 'nourishment',\n",
       " 'sufficiently',\n",
       " 'showtunes',\n",
       " 'hypoallergenic',\n",
       " 'varvatos',\n",
       " 'tnas',\n",
       " 'herbie',\n",
       " 'footloose',\n",
       " 'lukus',\n",
       " 'videolog',\n",
       " 'flowing',\n",
       " 'demonstrates',\n",
       " 'payout',\n",
       " 'teelook',\n",
       " 'hig',\n",
       " 'ngeremove',\n",
       " 'beltway',\n",
       " 'juga',\n",
       " 'ofd',\n",
       " 'diapointed',\n",
       " 'mote',\n",
       " 'citrix',\n",
       " 'diarrhoea',\n",
       " 'orr',\n",
       " 'brekky',\n",
       " 'educating',\n",
       " 'nagteach',\n",
       " 'islikes',\n",
       " 'penney',\n",
       " 'bastard',\n",
       " 'fridayamazing',\n",
       " 'denyhosts',\n",
       " 'fakin',\n",
       " 'disappointed',\n",
       " 'chittom',\n",
       " 'seasoning',\n",
       " 'plunger',\n",
       " 'mihuu',\n",
       " 'puple',\n",
       " 'pentiing',\n",
       " 'duh',\n",
       " 'shashed',\n",
       " 'enablers',\n",
       " 'tweetsters',\n",
       " 'headquarter',\n",
       " 'revitiligo',\n",
       " 'adulthood',\n",
       " 'yyukk',\n",
       " 'triplestrength',\n",
       " 'midday',\n",
       " 'xpac',\n",
       " 'straightthrough',\n",
       " 'donnington',\n",
       " 'savored',\n",
       " 'incisive',\n",
       " 'bitreach',\n",
       " 'chunker',\n",
       " 'wintercamp',\n",
       " 'sarabeths',\n",
       " 'feis',\n",
       " 'gettysburg',\n",
       " 'deision',\n",
       " 'navy',\n",
       " 'taj',\n",
       " 'kungfu',\n",
       " 'cm',\n",
       " 'clubhouse',\n",
       " 'mattitude',\n",
       " 'nguá',\n",
       " 'avut',\n",
       " 'hungrified',\n",
       " 'onnw',\n",
       " 'oneparamorefreaknamedjenna',\n",
       " 'pvr',\n",
       " 'cales',\n",
       " 'fletch',\n",
       " 'cá',\n",
       " 'bassinet',\n",
       " 'rebuilding',\n",
       " 'unappropriate',\n",
       " 'imishu',\n",
       " 'ricebunny',\n",
       " 'fttn',\n",
       " 'descriptio',\n",
       " 'nautique',\n",
       " 'requested',\n",
       " 'reciprocating',\n",
       " 'dominic',\n",
       " 'peezy',\n",
       " 'yeasterday',\n",
       " 'impersonated',\n",
       " 'morga',\n",
       " 'caillou',\n",
       " 'volterra',\n",
       " 'turku',\n",
       " 'dancefloor',\n",
       " 'fashionista',\n",
       " 'soto',\n",
       " 'diamondback',\n",
       " 'eyemelting',\n",
       " 'dontchu',\n",
       " 'exercise',\n",
       " 'bebee',\n",
       " 'presiding',\n",
       " 'tod',\n",
       " 'tinky',\n",
       " 'yesteryear',\n",
       " 'knocky',\n",
       " 'practisng',\n",
       " 'publish',\n",
       " 'lula',\n",
       " 'syringe',\n",
       " 'winksound',\n",
       " 'petticoat',\n",
       " 'rn',\n",
       " 'puk',\n",
       " 'documenting',\n",
       " 'window',\n",
       " 'hrmmph',\n",
       " 'ninmode',\n",
       " 'angwaagee',\n",
       " 'fiinee',\n",
       " 'stick',\n",
       " 'kswiss',\n",
       " 'rebook',\n",
       " 'anjum',\n",
       " 'prescribed',\n",
       " 'hyacinth',\n",
       " 'roseville',\n",
       " 'hunners',\n",
       " 'speeddating',\n",
       " 'thigns',\n",
       " 'beeryani',\n",
       " 'canecel',\n",
       " 'flasbacks',\n",
       " 'realer',\n",
       " 'monye',\n",
       " 'prettier',\n",
       " 'gife',\n",
       " 'fifa',\n",
       " 'motherdaughter',\n",
       " 'reenforce',\n",
       " 'dronw',\n",
       " 'abalone',\n",
       " 'vampire',\n",
       " 'izsy',\n",
       " 'alp',\n",
       " 'partayy',\n",
       " 'whym',\n",
       " 'lonngeesst',\n",
       " 'haydn',\n",
       " 'hbazaar',\n",
       " 'gotti',\n",
       " 'tweetahs',\n",
       " 'periodontist',\n",
       " 'huffpo',\n",
       " 'bedside',\n",
       " 'searching',\n",
       " 'hipps',\n",
       " 'underwear',\n",
       " 'february',\n",
       " 'cockle',\n",
       " 'earn',\n",
       " 'guitarplaying',\n",
       " 'exausto',\n",
       " 'finshed',\n",
       " 'rhine',\n",
       " 'peli',\n",
       " 'hawthorn',\n",
       " 'scarce',\n",
       " 'oborne',\n",
       " 'kiannas',\n",
       " 'rá',\n",
       " 'wheatgrass',\n",
       " 'beckys',\n",
       " 'wastee',\n",
       " 'wlang',\n",
       " 'erlyer',\n",
       " 'loopey',\n",
       " 'hedphones',\n",
       " 'beginnin',\n",
       " 'kittyfloss',\n",
       " 'hazy',\n",
       " 'enaknya',\n",
       " 'truckload',\n",
       " 'mansour',\n",
       " 'osi',\n",
       " 'solve',\n",
       " 'hotelie',\n",
       " 'deff',\n",
       " 'bogan',\n",
       " 'joses',\n",
       " 'seocnds',\n",
       " 'feat',\n",
       " 'dissected',\n",
       " 'starman',\n",
       " 'blh',\n",
       " 'essendon',\n",
       " 'single',\n",
       " 'measles',\n",
       " 'meditatemonday',\n",
       " 'nesbu',\n",
       " 'pissness',\n",
       " 'webcopy',\n",
       " 'criedagain',\n",
       " 'timmies',\n",
       " 'desais',\n",
       " 'firecracker',\n",
       " 'screened',\n",
       " 'frau',\n",
       " 'ocarina',\n",
       " 'worsed',\n",
       " 'bpfurniture',\n",
       " 'magento',\n",
       " 'wgn',\n",
       " 'launching',\n",
       " 'kgb',\n",
       " 'lather',\n",
       " 'refining',\n",
       " 'tuned',\n",
       " 'macwise',\n",
       " 'twitterbaaz',\n",
       " 'grinder',\n",
       " 'beejive',\n",
       " 'renewal',\n",
       " 'jogg',\n",
       " 'accused',\n",
       " 'exaj',\n",
       " 'jokoy',\n",
       " 'noonan',\n",
       " 'bahha',\n",
       " 'sims',\n",
       " 'gangland',\n",
       " 'shine',\n",
       " 'mashine',\n",
       " 'ahahahahahahahah',\n",
       " 'tawangtawa',\n",
       " 'cubies',\n",
       " 'unbearable',\n",
       " 'turf',\n",
       " 'heappss',\n",
       " 'tgiers',\n",
       " 'liferay',\n",
       " 'auction',\n",
       " 'wendys',\n",
       " 'rickenbacker',\n",
       " 'tangier',\n",
       " 'yippee',\n",
       " 'byen',\n",
       " 'life',\n",
       " 'conflicted',\n",
       " 'craig',\n",
       " 'bark',\n",
       " 'redesign',\n",
       " 'mychal',\n",
       " 'helluvit',\n",
       " 'released',\n",
       " 'velva',\n",
       " 'iamge',\n",
       " 'squeakerbox',\n",
       " 'pentax',\n",
       " 'lisbon',\n",
       " 'bgp',\n",
       " 'lester',\n",
       " 'bleehh',\n",
       " 'rammstein',\n",
       " 'slambook',\n",
       " 'fedak',\n",
       " 'quacky',\n",
       " 'paxton',\n",
       " 'sal',\n",
       " 'yuckkii',\n",
       " 'hiob',\n",
       " 'merfolk',\n",
       " 'dufftown',\n",
       " 'interuptions',\n",
       " 'notte',\n",
       " 'newnownext',\n",
       " 'bwoi',\n",
       " 'wahh',\n",
       " 'arsenal',\n",
       " 'incredibots',\n",
       " 'noavatar',\n",
       " 'bridge',\n",
       " 'bshop',\n",
       " 'photshoot',\n",
       " 'layying',\n",
       " 'crochet',\n",
       " 'psigenix',\n",
       " 'scottys',\n",
       " 'bonkers',\n",
       " 'rewatched',\n",
       " 'dieter',\n",
       " 'hecktic',\n",
       " 'trina',\n",
       " 'larping',\n",
       " 'tucson',\n",
       " 'fivestarred',\n",
       " 'choking',\n",
       " 'implement',\n",
       " 'tobad',\n",
       " 'savignon',\n",
       " 'itinery',\n",
       " 'sya',\n",
       " 'laceys',\n",
       " 'perut',\n",
       " 'couse',\n",
       " 'scolded',\n",
       " 'pavillion',\n",
       " 'exaggerated',\n",
       " 'cynny',\n",
       " 'kim',\n",
       " 'slum',\n",
       " 'bulk',\n",
       " 'qi',\n",
       " 'sibuk',\n",
       " 'yanktosky',\n",
       " 'transwarps',\n",
       " 'pelinkovac',\n",
       " 'francisco',\n",
       " 'ateneo',\n",
       " 'sertenity',\n",
       " 'basil',\n",
       " 'welkies',\n",
       " 'metrosphere',\n",
       " 'sllep',\n",
       " 'schmuck',\n",
       " 'decisive',\n",
       " 'simpson',\n",
       " 'ooow',\n",
       " 'singin',\n",
       " 'congradulations',\n",
       " 'loser',\n",
       " 'opictures',\n",
       " 'bwtn',\n",
       " 'vautrin',\n",
       " 'pukey',\n",
       " 'havn',\n",
       " 'epilepsy',\n",
       " 'hur',\n",
       " 'communique',\n",
       " 'multicoloured',\n",
       " 'georgeous',\n",
       " 'faning',\n",
       " 'stressing',\n",
       " 'tesla',\n",
       " 'weeps',\n",
       " 'bishes',\n",
       " 'ceebs',\n",
       " 'paola',\n",
       " 'marrier',\n",
       " 'aftenoon',\n",
       " 'ná',\n",
       " 'assigning',\n",
       " 'expenisve',\n",
       " 'carry',\n",
       " 'mouththen',\n",
       " 'baloon',\n",
       " 'fuuck',\n",
       " 'hallarious',\n",
       " 'market',\n",
       " 'plush',\n",
       " 'antomical',\n",
       " 'willingly',\n",
       " 'growingup',\n",
       " 'bolting',\n",
       " 'chameleon',\n",
       " 'scheduler',\n",
       " 'heartless',\n",
       " 'shobilitaa',\n",
       " 'favela',\n",
       " 'marino',\n",
       " 'crippled',\n",
       " 'tmz',\n",
       " 'sermon',\n",
       " 'sushi',\n",
       " 'brie',\n",
       " 'kjasdhfiashiure',\n",
       " 'preforming',\n",
       " 'tigaraksa',\n",
       " 'substitute',\n",
       " 'parakeet',\n",
       " 'speek',\n",
       " 'enrollment',\n",
       " 'gazz',\n",
       " 'olivesi',\n",
       " 'uppertunities',\n",
       " 'minusheart',\n",
       " 'declar',\n",
       " 'therescue',\n",
       " 'riku',\n",
       " 'behavioral',\n",
       " 'ghuys',\n",
       " 'relaxes',\n",
       " 'aikido',\n",
       " 'haill',\n",
       " 'frexx',\n",
       " 'fders',\n",
       " 'watchinn',\n",
       " 'freak',\n",
       " 'creslee',\n",
       " 'eitehr',\n",
       " 'whitecollar',\n",
       " 'monk',\n",
       " 'formed',\n",
       " 'burrard',\n",
       " 'hhi',\n",
       " 'chargeranother',\n",
       " 'stronng',\n",
       " 'supp',\n",
       " 'reduced',\n",
       " 'ahahahahahahahahaha',\n",
       " 'antirain',\n",
       " 'sicck',\n",
       " 'wrangling',\n",
       " 'huglove',\n",
       " 'showertimee',\n",
       " 'msk',\n",
       " 'hugely',\n",
       " 'seemed',\n",
       " 'effers',\n",
       " 'kristie',\n",
       " 'elated',\n",
       " 'youknowwho',\n",
       " 'cfeclipse',\n",
       " 'lovelove',\n",
       " 'element',\n",
       " 'aritzia',\n",
       " 'facebk',\n",
       " 'headoverheels',\n",
       " 'iti',\n",
       " 'loginbox',\n",
       " 'phonecalling',\n",
       " 'tenso',\n",
       " 'corgi',\n",
       " 'mouthful',\n",
       " 'audiobooks',\n",
       " 'maddy',\n",
       " 'zine',\n",
       " 'jambinese',\n",
       " 'precipitation',\n",
       " 'skamania',\n",
       " 'derry',\n",
       " 'ohmygawsh',\n",
       " 'fiya',\n",
       " 'fusicology',\n",
       " 'tatts',\n",
       " 'unpardonable',\n",
       " 'bragg',\n",
       " 'mass',\n",
       " 'camilo',\n",
       " 'keating',\n",
       " 'graspop',\n",
       " 'carded',\n",
       " 'pushin',\n",
       " 'sharian',\n",
       " 'corndog',\n",
       " 'isnit',\n",
       " 'jens',\n",
       " 'brelubener',\n",
       " 'predicted',\n",
       " 'facebookrelationshipstatuschange',\n",
       " 'frail',\n",
       " 'stack',\n",
       " 'deessaa',\n",
       " 'sparkledazzle',\n",
       " 'discus',\n",
       " 'noite',\n",
       " 'ljs',\n",
       " 'wakeboard',\n",
       " 'nashiess',\n",
       " 'gilligans',\n",
       " 'genoa',\n",
       " 'omnomnomnomnom',\n",
       " 'twweets',\n",
       " 'income',\n",
       " 'copay',\n",
       " 'iont',\n",
       " 'tofu',\n",
       " 'gor',\n",
       " 'itchy',\n",
       " 'zap',\n",
       " 'gini',\n",
       " 'consevatory',\n",
       " 'gazing',\n",
       " 'bacon',\n",
       " 'capri',\n",
       " 'questies',\n",
       " 'alize',\n",
       " 'ptsd',\n",
       " 'bahahahahaha',\n",
       " 'banzai',\n",
       " 'beeasy',\n",
       " 'lifee',\n",
       " 'touse',\n",
       " 'hossie',\n",
       " 'lustrous',\n",
       " 'saunders',\n",
       " 'spacemunkey',\n",
       " 'brunei',\n",
       " 'dreamingg',\n",
       " 'eas',\n",
       " 'jilldecoy',\n",
       " 'soir',\n",
       " 'montue',\n",
       " 'xjoeyx',\n",
       " 'reporting',\n",
       " 'jetsetter',\n",
       " 'wic',\n",
       " 'zendesk',\n",
       " 'wubi',\n",
       " 'peedi',\n",
       " 'deborah',\n",
       " 'qepd',\n",
       " 'notify',\n",
       " 'ecnn',\n",
       " 'eventough',\n",
       " 'fams',\n",
       " 'bacckk',\n",
       " 'koreai',\n",
       " 'shazzyfizzle',\n",
       " 'midol',\n",
       " 'tweetable',\n",
       " 'liikod',\n",
       " 'jimi',\n",
       " 'locaioin',\n",
       " 'wordplay',\n",
       " 'painting',\n",
       " 'arclight',\n",
       " 'megga',\n",
       " 'optician',\n",
       " 'dormant',\n",
       " 'rice',\n",
       " 'ya',\n",
       " 'choda',\n",
       " 'whem',\n",
       " 'vinel',\n",
       " 'chagaev',\n",
       " 'grobie',\n",
       " 'nizzle',\n",
       " 'ony',\n",
       " 'picknmix',\n",
       " 'ginny',\n",
       " 'nutha',\n",
       " 'wacki',\n",
       " 'sambergs',\n",
       " 'hsband',\n",
       " 'finishing',\n",
       " 'listless',\n",
       " 'secondary',\n",
       " 'permanantly',\n",
       " 'muhahahaha',\n",
       " 'upstate',\n",
       " 'nelnet',\n",
       " 'kurosawa',\n",
       " 'freerunner',\n",
       " 'predictably',\n",
       " 'halfempty',\n",
       " 'interviewer',\n",
       " 'againtill',\n",
       " 'jarred',\n",
       " 'migranes',\n",
       " 'bbeeauutiiffull',\n",
       " 'rino',\n",
       " 'paypal',\n",
       " 'qoosfr',\n",
       " 'projekts',\n",
       " 'thrus',\n",
       " 'twittr',\n",
       " 'fawcett',\n",
       " 'thingy',\n",
       " 'signup',\n",
       " 'boxen',\n",
       " 'uipii',\n",
       " 'neyce',\n",
       " 'greta',\n",
       " 'humn',\n",
       " 'goatee',\n",
       " 'hahaha',\n",
       " 'rele',\n",
       " 'eloped',\n",
       " 'bloomin',\n",
       " 'hiss',\n",
       " 'klonopin',\n",
       " 'blackcurrant',\n",
       " 'cosigning',\n",
       " 'melai',\n",
       " 'hoye',\n",
       " 'jondo',\n",
       " 'frick',\n",
       " 'enjoyablee',\n",
       " 'louby',\n",
       " 'temen',\n",
       " 'blogasty',\n",
       " 'r',\n",
       " 'vienna',\n",
       " 'palor',\n",
       " 'heydemi',\n",
       " 'garbo',\n",
       " 'hut',\n",
       " 'coughcoughlyingcough',\n",
       " 'dragonball',\n",
       " 'chane',\n",
       " 'susceptible',\n",
       " 'success',\n",
       " 'cnu',\n",
       " 'eczema',\n",
       " 'leavin',\n",
       " 'bassingthwaighte',\n",
       " 'suckky',\n",
       " 'adtech',\n",
       " 'peform',\n",
       " 'blindly',\n",
       " 'rop',\n",
       " 'smthing',\n",
       " 'micartney',\n",
       " 'dweeter',\n",
       " 'rugrats',\n",
       " 'brick',\n",
       " 'grecian',\n",
       " 'appster',\n",
       " 'madysons',\n",
       " 'blck',\n",
       " 'goldd',\n",
       " 'kdawg',\n",
       " 'jonandkate',\n",
       " 'henne',\n",
       " 'goldberg',\n",
       " 'twitfam',\n",
       " 'improper',\n",
       " 'jeah',\n",
       " 'watchingg',\n",
       " 'soundcheck',\n",
       " 'leonda',\n",
       " 'godfather',\n",
       " 'iroc',\n",
       " 'uwe',\n",
       " 'michaelson',\n",
       " 'rufus',\n",
       " 'task',\n",
       " 'nem',\n",
       " 'knoww',\n",
       " 'sinclar',\n",
       " 'steves',\n",
       " 'tackled',\n",
       " 'calo',\n",
       " 'goose',\n",
       " 'piatu',\n",
       " 'nobritish',\n",
       " 'boriing',\n",
       " 'populationn',\n",
       " 'tta',\n",
       " 'collected',\n",
       " 'errbody',\n",
       " 'soarthroat',\n",
       " 'shylana',\n",
       " 'outragedno',\n",
       " 'kinetic',\n",
       " 'khao',\n",
       " 'telecast',\n",
       " 'jaci',\n",
       " 'nordestino',\n",
       " 'sooner',\n",
       " 'mockinterview',\n",
       " 'wla',\n",
       " 'foolow',\n",
       " 'lightwood',\n",
       " 'shuffle',\n",
       " 'sankofa',\n",
       " 'uncw',\n",
       " 'anglosaxon',\n",
       " 'organization',\n",
       " 'durant',\n",
       " 'holidez',\n",
       " 'burlingame',\n",
       " 'crawlin',\n",
       " 'arguing',\n",
       " 'bandwidth',\n",
       " 'postpone',\n",
       " 'bffl',\n",
       " 'moovie',\n",
       " 'tak',\n",
       " 'acorntree',\n",
       " 'stephanies',\n",
       " 'fuk',\n",
       " 'vola',\n",
       " 'muzzled',\n",
       " 'immobily',\n",
       " 'nero',\n",
       " 'bev',\n",
       " 'hosptal',\n",
       " 'deadman',\n",
       " 'mcfaddens',\n",
       " 'vlach',\n",
       " 'nissan',\n",
       " 'rebandaged',\n",
       " 'milo',\n",
       " 'molesting',\n",
       " 'rindu',\n",
       " 'eorld',\n",
       " 'goodness',\n",
       " 'anchovy',\n",
       " 'omission',\n",
       " 'oppresive',\n",
       " 'tubage',\n",
       " 'inti',\n",
       " 'committing',\n",
       " 'followrs',\n",
       " 'shultz',\n",
       " 'etsy',\n",
       " 'fggkjfkjdfjs',\n",
       " 'yg',\n",
       " 'rippen',\n",
       " 'hbk',\n",
       " 'rested',\n",
       " 'bios',\n",
       " 'sombody',\n",
       " 'ubergenius',\n",
       " 'minogue',\n",
       " 'loyola',\n",
       " 'jongil',\n",
       " 'bouncewithme',\n",
       " 'aaa',\n",
       " 'tablet',\n",
       " 'tenacious',\n",
       " 'gogrid',\n",
       " 'atwood',\n",
       " 'finchley',\n",
       " 'jelsa',\n",
       " 'norff',\n",
       " 'hahasam',\n",
       " 'loonnerr',\n",
       " 'milk',\n",
       " 'bestt',\n",
       " 'awaits',\n",
       " 'discussing',\n",
       " 'density',\n",
       " 'rainy',\n",
       " 'haizz',\n",
       " 'jcp',\n",
       " 'nietzsche',\n",
       " 'simpatika',\n",
       " 'joonas',\n",
       " 'examiner',\n",
       " 'hopingnfor',\n",
       " 'karaokeing',\n",
       " 'bfe',\n",
       " 'killarney',\n",
       " 'mmg',\n",
       " 'sleppt',\n",
       " 'timeouts',\n",
       " 'jeoung',\n",
       " 'tranformers',\n",
       " 'futuristic',\n",
       " 'mannareo',\n",
       " 'tsel',\n",
       " 'borought',\n",
       " 'exctly',\n",
       " 'peopleess',\n",
       " 'tamarack',\n",
       " 'pittsburg',\n",
       " 'preg',\n",
       " 'exciteded',\n",
       " 'mex',\n",
       " 'sepi',\n",
       " 'acturly',\n",
       " 'tracker',\n",
       " 'deal',\n",
       " 'disrupted',\n",
       " 'strightening',\n",
       " 'smtb',\n",
       " 'manhatton',\n",
       " 'minit',\n",
       " 'nittee',\n",
       " 'worldofgoo',\n",
       " 'coloured',\n",
       " 'howre',\n",
       " 'northridge',\n",
       " 'korres',\n",
       " 'ckin',\n",
       " 'shrunk',\n",
       " 'recalling',\n",
       " 'foodtrip',\n",
       " 'ghalbum',\n",
       " 'housetrained',\n",
       " 'payton',\n",
       " 'anyy',\n",
       " 'kyte',\n",
       " 'shitouse',\n",
       " 'sinai',\n",
       " 'chocolaty',\n",
       " 'grissum',\n",
       " 'jira',\n",
       " 'randwick',\n",
       " 'varity',\n",
       " 'longleat',\n",
       " 'sadexposed',\n",
       " 'berto',\n",
       " 'interhouse',\n",
       " 'cablecom',\n",
       " 'fawecett',\n",
       " 'tweetlaxing',\n",
       " 'tmro',\n",
       " 'pdhl',\n",
       " 'tokio',\n",
       " 'waait',\n",
       " 'lend',\n",
       " 'eationg',\n",
       " 'gamla',\n",
       " 'lvling',\n",
       " 'swede',\n",
       " 'fareal',\n",
       " 'dont',\n",
       " 'yom',\n",
       " 'fooled',\n",
       " 'procrastinated',\n",
       " 'otea',\n",
       " 'vasily',\n",
       " 'stragetist',\n",
       " 'blackbeerry',\n",
       " 'nandys',\n",
       " 'twtad',\n",
       " 'adequate',\n",
       " 'assface',\n",
       " 'rell',\n",
       " 'tiwit',\n",
       " 'cheezit',\n",
       " 'instance',\n",
       " 'reproduce',\n",
       " 'absolut',\n",
       " 'ballon',\n",
       " 'three',\n",
       " 'dawas',\n",
       " 'christianis',\n",
       " 'recordins',\n",
       " 'unfortunatelly',\n",
       " 'charctr',\n",
       " 'wellz',\n",
       " 'phonecall',\n",
       " 'combomint',\n",
       " 'umplings',\n",
       " 'twittergadget',\n",
       " 'jugular',\n",
       " 'py',\n",
       " 'saltville',\n",
       " 'bside',\n",
       " 'mailbox',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_set = set(word_set)\n",
    "word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "829f4c96-e106-4e38-874a-3cd900ca351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(requiredTweetData)\n",
    "total_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cd182c1-d9a4-4e6f-8eb1-2b225b5a7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an index for each word in our vocab.\n",
    "index_dict = {} #Dictionary to store index for each word\n",
    "i = 0\n",
    "for word in word_set:\n",
    "    index_dict[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7490e9b-3c37-4fe9-8d58-78af5bc476b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a count dictionary\n",
    " \n",
    "def count_dict(sentences):\n",
    "    word_count = {}\n",
    "    for word in word_set:\n",
    "        word_count[word] = 0\n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                word_count[word] += 1\n",
    "    return word_count\n",
    " \n",
    "word_count = count_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2081f8ae-c517-47d2-b93a-df16473c3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beee9d41-8e60-48a8-acb3-77cd071e7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency\n",
    " \n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_documents/word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e36c5a6-9a64-4272-962a-82424a121d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    tf_idf_vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = termfreq(sentence,word)\n",
    "        idf = inverse_doc_freq(word)\n",
    "         \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[index_dict[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e17a11c-9323-4c6a-a184-efb18125011c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m----> 4\u001b[0m     vec \u001b[38;5;241m=\u001b[39m \u001b[43mtf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     vectors\u001b[38;5;241m.\u001b[39mappend(vec)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectors[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn [31], line 5\u001b[0m, in \u001b[0;36mtf_idf\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m      4\u001b[0m     tf \u001b[38;5;241m=\u001b[39m termfreq(sentence,word)\n\u001b[1;32m----> 5\u001b[0m     idf \u001b[38;5;241m=\u001b[39m \u001b[43minverse_doc_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m*\u001b[39midf\n\u001b[0;32m      8\u001b[0m     tf_idf_vec[index_dict[word]] \u001b[38;5;241m=\u001b[39m value \n",
      "Cell \u001b[1;32mIn [30], line 8\u001b[0m, in \u001b[0;36minverse_doc_freq\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     word_occurance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtotal_documents\u001b[49m\u001b[38;5;241m/\u001b[39mword_occurance)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_documents' is not defined"
     ]
    }
   ],
   "source": [
    "#TF-IDF Encoded text corpus\n",
    "vectors = []\n",
    "for sent in sentences:\n",
    "    vec = tf_idf(sent)\n",
    "    vectors.append(vec)\n",
    "\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada7cec-457a-43bd-8a09-2a6397c9df9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
